<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2Funcategorized%2Fresnet%2F%E6%96%B0%E5%BB%BA%E6%96%87%E6%9C%AC%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[0. 前言说到残差网络ResNet，大家应该都不默认。近几年，深度学习领域的论文井喷，注重benchmark的文章不断强调自己把精度提高了多少多少点，注重算法思想的文章则宣称自己的算法多么多么有效。 但是这些文章中，鱼龙混杂，或者夸大的成分太多，我们该如何判断？工业界给了我们答案。如果一篇文章提到的思想能在工业被广泛使用的话，那一定是篇好文章。ResNet就是其中之一。 ResNet由微软亚研院MSRA的几位大佬所写，一发表，就引起巨大关注。就我个人的看法，ResNet真的是神作，在当时大家对一层一层堆叠的网络形成思维惯性的时候，shortcut的思想真的很厉害。 100+层的网络，运算量却和16层的VGG差不多，精度提高一个档次。而且模块性、可移植性很强。默默膜拜下大神。 1. ResNet网络上介绍ResNet的文章很多，比如：你必须要知道CNN模型：ResNet-小白将的文章 就我个人的看法，ResNet的核心就是shortcut的出现。图1是ResNet的基本模块。 【图1】 图1如果去掉旁路，那就是经典的堆叠式的网络。ResNet的贡献就是引入了旁路（shortcut）。旁路（shortcut）的引入一方面使得梯度的后向传播更加容易，使更深的网络得以有效训练；另一方面，也使得每个基本模块的学习任务从学习一个绝对量变为学习相对上一个基本模块的偏移量。 ResNet的网络结构很简单，就是把这些基本模块堆起来，形成不同层数的网络。基本模块的简单堆叠也使得ResNet可以很轻松地移植到其他网络中。图2是论文里提到的几种结构，它们都是不同数量的基本模块叠加得到的。 【图2】 还有一点令人惊讶的是，ResNet100+层的网络，运算量却和16层的VGG差不多，关于这点，可以看我之前的文章：【】 2. ResNeXtResNeXt是这个系列的新文章，是ResNet的升级版，升级内容为引入Inception的多支路的思想。 同样，网络上也有非常好的解读的文章：深度学习——分类之ResNeXt-范星.xfanplus的文章 如果你对ResNet和Inception都比较熟悉的话，那么其实只要看图3就能明白ResNeXt在做什么了。 【图3】 图3中左图是ResNet的基本模块，右图是ResNeXt的基本模块。容易发现，ResNeXt就是把ResNet的单个卷积改成了多支路的卷积。 作者说ResNeXt的目的是为了保持ResNet的高可移植性优点，同时继续提高精度。文章中还顺便吐槽了Inception的缺点。也就是我在上一篇介绍Inception的文章中说的【】：Inception系列的前两篇文章很惊艳，但是后两篇文章炼丹痕迹越来越重。直到Xception出现后，可移植性增强不少。]]></content>
  </entry>
  <entry>
    <title><![CDATA[聊一聊ResNet系列（ResNet、ResNeXt）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fresnet%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：终于到了介绍ResNet系列的时候了。ResNet真的很好用，特别时shortcut的想法真的厉害。最近，又推出了改进版本ResNeXt，新版本结合了Inception多支路的思想。 关键字：深度学习, ResNet, ResNeXt 0. 前言说到残差网络ResNet，大家应该都不默认。近几年，深度学习领域的论文井喷，注重benchmark的文章不断强调自己把精度提高了多少多少点，注重算法思想的文章则宣称自己的算法多么多么有效。 但是这些文章中，鱼龙混杂，或者夸大的成分太多，我们该如何判断？工业界给了我们答案。如果一篇文章提到的思想能在工业被广泛使用的话，那一定是篇好文章。ResNet就是其中之一。 ResNet由微软亚研院MSRA的几位大佬所写，一发表，就引起巨大关注。就我个人的看法，ResNet真的是神作，在当时大家对一层一层堆叠的网络形成思维惯性的时候，shortcut的思想真的是跨越性的。 你能想到100+层的网络，运算量却和16层的VGG差不多，精度提高一个档次。而且模块性、可移植性很强。默默膜拜下大神。 1. ResNet网络上介绍ResNet的文章很多，比如：你必须要知道CNN模型：ResNet-小白将的文章 就我个人的看法，ResNet的核心就是shortcut的出现。图1是ResNet的基本模块。 图1 图1如果去掉旁路，那就是经典的堆叠式的网络。ResNet的贡献就是引入了旁路（shortcut）。旁路（shortcut）的引入一方面使得梯度的后向传播更加容易，使更深的网络得以有效训练；另一方面，也使得每个基本模块的学习任务从学习一个绝对量变为学习相对上一个基本模块的偏移量。 ResNet的网络结构很简单，就是把这些基本模块堆起来，形成不同层数的网络。基本模块的简单堆叠也使得ResNet可以很轻松地移植到其他网络中。图2是论文里提到的几种结构，它们都是不同数量的基本模块叠加得到的。 图2 还有一点令人惊讶的是，ResNet100+层的网络，运算量却和16层的VGG差不多，关于这点，可以看我之前的文章：以VGG为例，分析深度网络的计算量和参数量 默默放上一段用Keras写的ResNet，如果熟悉Keras的话，相信通过代码能很快理解ResNet的结构。当然，现在ResNet已经被Keras内置，只需要一句代码就能写出ResNet。 代码里说的conv_block和identity_block其实就是ResNet的基本模块，它们的区别是conv_block的旁路是直接一条线，identity_block的旁路有一个卷积层。之所以有的基本模块旁路一条线，有的基础模块旁路会有卷积层，是为了保证旁路出来的featuremap和主路的featuremap尺寸一致，这样它们才能相加。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107############################################################# Resnet Graph############################################################# Code adopted from:# https://github.com/fchollet/deep-learning-models/blob/master/resnet50.pydef resnet_graph(input_image, architecture, stage5=False): assert architecture in ["resnet50", "resnet101"] # Stage 1 x = KL.ZeroPadding2D((3, 3))(input_image) x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x) x = BatchNorm(axis=3, name='bn_conv1')(x) x = KL.Activation('relu')(x) C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding="same")(x) # Stage 2 x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1)) x = identity_block(x, 3, [64, 64, 256], stage=2, block='b') C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c') # Stage 3 x = conv_block(x, 3, [128, 128, 512], stage=3, block='a') x = identity_block(x, 3, [128, 128, 512], stage=3, block='b') x = identity_block(x, 3, [128, 128, 512], stage=3, block='c') C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d') # Stage 4 x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a') block_count = &#123;"resnet50": 5, "resnet101": 22&#125;[architecture] for i in range(block_count): x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i)) C4 = x # Stage 5 if stage5: x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a') x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b') C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c') else: C5 = None return [C1, C2, C3, C4, C5]def identity_block(input_tensor, kernel_size, filters, stage, block, use_bias=True): """The identity_block is the block that has no conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: defualt 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names """ nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(axis=3, name=bn_name_base + '2a')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2b')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2c')(x) x = KL.Add()([x, input_tensor]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return xdef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True): """conv_block is the block that has a conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: defualt 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names Note that from stage 3, the first conv layer at main path is with subsample=(2,2) And the shortcut should have subsample=(2,2) as well """ nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(axis=3, name=bn_name_base + '2a')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2b')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2c')(x) shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', use_bias=use_bias)(input_tensor) shortcut = BatchNorm(axis=3, name=bn_name_base + '1')(shortcut) x = KL.Add()([x, shortcut]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x 2. ResNeXtResNeXt是这个系列的新文章，是ResNet的升级版，升级内容为引入Inception的多支路的思想。 同样，网络上也有非常好的解读的文章：深度学习——分类之ResNeXt-范星.xfanplus的文章 如果你对ResNet和Inception都比较熟悉的话，那么其实只要看图3就能明白ResNeXt在做什么了。 图3 图3中左图是ResNet的基本模块，右图是ResNeXt的基本模块。容易发现，ResNeXt就是把ResNet的单个卷积改成了多支路的卷积。 作者说ResNeXt的目的是为了保持ResNet的高可移植性优点，同时继续提高精度。文章中还顺便吐槽了Inception的缺点。也就是我在上一篇介绍Inception的文章（聊一聊Inception系列）中说的：Inception系列的前两篇文章很惊艳，但是后两篇文章炼丹痕迹越来越重。直到Xception出现后，可移植性增强不少。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>ResNet</tag>
        <tag>ResNeXt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多种子的区域生长算法]]></title>
    <url>%2F%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2Fmulti-seed-region-grow%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：多种子的区域生长算法，基于C++编写。 关键字：图像处理, 种子生长, 区域生长 1. 题外话最近需要找一种简单对图像进行分割的算法，作为后续算法的前处理阶段。最开始想到的是聚类，但是聚类会有分割后不保证连通性的问题。 区域生长法可以保证分割后各自区域的连通性。但网上大多数的代码都是单个种子的，用的多是matlab或旧版本的opencv。索性，我照着单种子的思路，写了一个多种子的区域生长算法分享出来。 2. 单种子的区域生长单种子的区域生长可以看这篇文章：图像处理算法1——区域生长法-夏天的风 事实上，我就是参照这篇文章的思路写的代码。这篇文章的代码思路清晰，可惜用的旧版本的opencv，而且是单种子。 3. 多种子的区域生长大体思路是这样的： 1、遍历全图，寻找是否还有undetermined的点，如有，作为种子 2、进行单种子的区域生长算法，生长出的区域记入矩阵mask 3、如果mask记录的区域面积足够大，把mask中的区域记录到矩阵dest中，状态为determined；如果mask记录的区域面积太小，把mask中的区域记录到矩阵dest中，状态为ignored（忽略分割出来面积过小的区域） 4、重复上述步骤，直到全图不存在undetermined的点 4. TODO我对现在的算法还不是很满意，其一，我没有做太多优化，代码运行卡卡的；其二，我用的灰度图，以及“差值/阈值”的方式来判断是否相似，阈值不是自适应的。 对于图片尺寸越大越卡这个问题，可以在读图片的时候统一resize成256×256，大多数追求速度的图像处理算法都会有这步。 对于彩色图，用3个通道的距离来判断相似度会更好。 以上两个问题，我为了保持算法的原汁原味，就没有添加。（其实是我懒得写了～） 5. 代码详见Github：https://github.com/imLogM/multi_seed_region_grow 6. 效果 原图 结果]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>种子生长</tag>
        <tag>区域生长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++常见问题（持续更新）]]></title>
    <url>%2FC-C%2Fc-problem-1%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：记录在使用C/C++时遇到的常见问题：友元，计时，cout与printf，结构体内存对齐，虚函数，派生类，auto关键字，指针的sizeof 关键字：C/C++ 1. 友元关于友元函数，可以看这篇文章：【C++基础之十】友元函数和友元类-偶尔e网事 文章里说的友元的缺点是破坏封装性比较好理解，友元的优点是提高程序运行效率这点我没有太想明白。（为什么我感觉友元的优点是让程序员能少写一点代码？） 文章最后提到友元不具有继承性和传递性，记一下就好。实际编程时，我还没有遇到使用友元的情况。 2. 计时在调试程序的时候，我们经常需要记录某一段代码运行的时间。我一般是使用clock()来计时的。示例代码如下：1234567891011#include &lt;ctime&gt; // 必须添加的头文件，C++是ctime，C是time.hint main() &#123; clock_t start = clock(); // clock_t 其实是long int for (int i=0; i&lt;1000000; i++); // 需要计时的程序段 clock_t end = clock(); double time_used = 1.0 * (end - start) / CLOCKS_PER_SEC; printf("程序用时： %.5f秒", time_used); return 0;&#125; 如果有过硬件开发经验的同学应该能明白，函数clock()获取的是滴答计数器的值。可以形象地理解：计算机的底层（或硬件或软件模拟）能不断地发出滴答声，并且滴答声的频率是稳定的，每秒发出CLOCKS_PER_SEC次滴答声。程序一开始运行，就有一个计数器启动，不断记录它听到的滴答声的次数。clock()获取到的就是这个计数器的值。clock() / CLOCKS_PER_SEC得到的就是从程序开始运行到当前经过几秒。 在我的电脑上，CLOCKS_PER_SEC=1000000，类型clock_t是long int的别名，在不同的硬件上，这些值不一定相同。 注意：这种计时方式有最大可计时时间。我们可以算一下，long int型最大为$2^{31}$，$2^{31} \div 1000000=2147$，2147秒大约是35分钟。也就是说程序运行后，每过大约35分钟，clock()取到的值会再次回到0。网上也有教程说最大计时时间为72分钟，可能是当成了unsigned long在算了。 3. cout与printfcout比printf要方便一点，因为可以不用写格式控制字符串%d、%s等。重载运算符后也能让cout输出非基本类型的变量。 但是printf比cout在IO效率上要更高一些，当然也有方法可以弥补这点，可以看这篇文章：cin与scanf cout与printf效率问题 4. 结构体内存对齐这个涉及到比较底层的问题，用来解释sizeof(结构体)得到的结果为什么和直观想的不一样。可以看这篇文章：如何理解 struct 的内存对齐？-Courtier的回答 注意，形似下方代码的东西不是普通结构体，是位域，它的内存占用分析更复杂，这里就不展开了： 123456struct bs // 位域&#123; int a:8; // 8表示占用8个bit int b:2; int c:6;&#125;; 5. 虚函数定义一个函数为虚函数，不代表函数为不被实现的函数。定义他为虚函数是为了允许用基类的指针来调用子类的这个函数。 定义一个函数为纯虚函数，才代表函数没有被实现。纯虚函数定义如下：1virtual void fun()=0; 6. 关于派生类 公有继承的公有成员还是公有的，可以被访问 公有继承的私有成员不被继承，所以不能访问 公有继承的保护成员可以被类的方法访问，不能被对象访问 私有继承的公有成员会变成派生类的私有成员，也不能被访问 7. auto关键字auto关键字在C++11以后有了不同于以前的含义。以前，auto关键字用来表示该变量是具有自动存储期的局部变量。如下面代码所示： 1234// C++11标准以前，auto关键字的用法int a; // 平时，auto关键字可省略auto int a; // auto表示有自动存储期的局部变量，与上句效果相同static int a; // 静态变量没有自动存储器 C++11以后，auto表示该变量的类型由编译器推理。如下面代码所示： 123// C++11标准以后int a; auto b = a; // b没有类型，但编译器会自动推理出b的类型为int 新标准下的auto主要在定义STL的迭代器时用到，因为这些迭代器类型名字都太长了（比如std::vector&lt;std::string&gt;::iterator），使用auto关键字可以少打很多字。 8. 指针的sizeof这其实是一个值得注意的地方，因为老教材通常默认是32位机器。32位程序，sizeof(指针)=4；64位程序，sizeof(指针)=8 12345678#include &lt;iostream&gt;int main() &#123; int a = 0; int* p = &amp;a; std::cout &lt;&lt; sizeof(p) &lt;&lt; std::endl; return 0;&#125;]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊Inception系列（GoogLeNet、Inception、Xception）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Finception%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：最近使用Xception时发现效果很好，所以打算介绍下整个Inception系列。 关键字：深度学习, Inception, Xception, GoogLeNet 1. 从模型结构说起其实关于Inception的结构，以及各代的改进，大家可以看这篇文章：深入浅出——网络模型中Inception的作用与结构全解析-深度学习思考者 有同学可能要说，上面链接里的那篇文章这么简单，似乎没讲太多内容。其实因为Inception每代之间联系性比较强，所以看明白了其中一篇，其他的也都能很快懂。如果要我来讲的话，大概会是下面四行字： GoogLeNet（Inception-v1）：相比AlexNet和VGG，出现了多支路，引入了1×1卷积帮助减少网络计算量 Inception-v2：引入Batch Normalization(BN)；5×5卷积使用两个3×3卷积代替 Inception-v3：n×n卷积分割为1×n和n×1两个卷积 Inception-v4：进一步优化，引入ResNet的shortcut思想 个人体会：v1和v2中的改进对深度学习的发展具有非常大的意义；v3有一点创新，但开始出现“炼丹”的感觉；v4参考ResNet说明ResNet的思想确实牛逼，其他部分完全在“炼丹”，网络过于精细，不容易迁移到其他任务中 推荐大家在训练模型时尝试使用Inception-v2中出现的Batch Normalization(BN)，关于BN的原理，可以看我上一篇文章：Batch Normalization(BN) 2. Xception之所以把Xception单拎出来说，一是因为Xception比较新，上面那篇文章没有讲到；二是因为Xception设计的目的与Inception不同：Inception的目标是针对分类任务追求最高的精度，以至于后面几代开始“炼丹”，模型过于精细；Xception的目标是设计出易迁移、计算量小、能适应不同任务，且精度较高的模型。 那么Xception与Inception-v3在结构上有什么差别呢？ 如图1为Inception-v3的模块结构，依据化繁为简的思想，把模块结构改造为图2。 图1 图2 依据depthwise separable convolution的思想，可以进一步把图2改造到图3。 图3 什么是depthwise separable convolution呢？MobileNet等网络为了减少计算量都有用到这个方法，不过Xception在这里的用法和一般的depthwise separable convolution还有点不同，所以为了防止大家搞糊涂，我就不介绍一般性的用法了，直接介绍Xception中的用法。 对于112×112×64的输入做一次普通的3×3卷积，每个卷积核大小为3×3×64，也就是说，每一小步的卷积操作是同时在面上（3×3的面）和深度上（×64的深度）进行的。 那如果把面上和深度上的卷积分离开来呢？这就是图3所要表达的操作。依旧以112×112×64的输入来作例子，先进入1×1卷积，每个卷积核大小为1×1×64，有没有发现，这样每一小步卷积其实相当于只在深度上（×64的深度）进行。 然后，假设1×1卷积的输出为112×112×7，我们把它分为7份，即每份是112×112×1，每份后面单独接一个3×3的卷积（如图3所示，画了7个3×3的框），此时每个卷积核为3×3×1，有没有发现，这样每一小步卷积其实相当于只在面上（3×3的面）进行。 最后，把这7个3×3的卷积的输出叠在一起就可以了。根据Xception论文的实验结果，Xception在精度上略低于Inception-v3，但在计算量和迁移性上都好于Inception-v3。 3. 关于模型复杂度的计算其实这部分内容我本来打算重点写的，为此我还特地写了篇“以VGG为例，分析模型复杂度”的文章。(+﹏+)~ 可是，就在写那篇文章的时候，我发现了一篇对Inception的复杂度计算介绍得非常清晰的文章。那我就偷个懒，大家可以直接看这篇文章的后半部分：卷积神经网络的复杂度分析-Michael Yuan的文章]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Inception</tag>
        <tag>Xception</tag>
        <tag>GoogLeNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Normalization (BN)]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fbatch-normalization%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：BN最早出现在Google的Inception(v2)网络中，它的效果让人瞠目结舌。那么，BN的原理是什么？为什么会有这么好的效果？ 关键字：深度学习, BN, 优化 1. 从数据预处理谈起如果你有相关的机器学习经验的话，应该知道在进行模型训练前需要进行数据预处理。而数据预处理中非常重要的一步是“归一化”（Normalization）。 为什么需要归一化呢？其一，因为机器学习的训练本质上是让模型学习到数据的分布。如果训练集的分布与测试集的分布有很大不同，那么训练出来的模型泛化能力就差，人们自然希望先把训练集和测试集分布调整到相近再训练；其二，实践表明，一个好的数据分布，有利于加速模型的训练及减少不收敛出现的概率，人们自然希望把数据调整到“好的分布”再训练。 怎样的分布算是好的分布？如图1左小图，数据分布的中心不在原点，数据分布的x方向明显长于y方向，这不是一个好的分布；如图1右小图，数据分布的中心在原点，数据分布的各个方向长度都接近1，这是一个好的分布。当然，并不是说图1的分布不能用于训练，只要训练集和测试集的分布相同，无论怎样差的分布，理论上都能训练出具有优秀泛化能力的模型；只是分布越差，训练起来越困难。 图1 左中右分别代表：原图、PCA后、白化后的数据分布 在数据预处理中归一化常用的方法是主成分分析( PCA )与 白化( whitening )。它们的作用，就是把数据变到“好的分布”下。如果不了解这两项技术，可以看这篇文章：主成分分析( PCA )与 白化( whitening )-Pony_s 2. 预处理还不够也许数据预处理在机器学习领域还够用，但是到了深度学习领域，就显得力不从心了：因为深度网络很深，每一层过后，数据的分布都会产生一定的变化；浅层的微小变化经过一层层放大，到了深层就是很大的变化了。 论文把这样现象叫做“Internal Covariate Shift”。 如果能有一种方法，能够在每一层的输出上进行数据归一化就好了。Batch Normalization正是要解决这个问题。 3. Batch Normalization公式：$$\widehat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$ 如何理解这个公式？结合图1，先看分子部分，$E[x^{(k)}]$表示的是$x^{(k)}$的期望，也就是平均值。$x^{(k)}-E[x^{(k)}]$，每个数据减去整体平均值，那么得到数据关于原点对称，这步的作用就是使数据分布关于原点对称。 再看分母部分，$Var[x^{(k)}]$表示求数据的方差，$\sqrt{Var[x^{(k)}]}$表示数据的标准差。分子除分母之后，数据的分布在各个方向上的标准差都为1，所以这步的作用是使数据分布在各个方向的标准差都为1。 4. 没有那么简单事实情况还没有那么简单。要知道并不是无脑在每一层的输出都加上上面的公式就能得到好的结果。万一某一些层就是要特殊的数据分布怎么办？我们不如把这个问题交给深度网络自身。 $$y^{(k)}=\gamma ^{(k)}\widehat{x}^{(k)}+\beta ^{(k)}$$ 公式中$\gamma ^{(k)}$和$\beta ^{(k)}$都是通过网络学习得到的。不难想到，当：$$\gamma ^{(k)}=\sqrt{Var[x^{(k)}]}\space ，\space \space \beta ^{(k)}=E[x^{(k)}]$$ 相当于没有加入归一化。 关于Batch Normalization的进一步展开，可以看这篇文章：Batch Normalization 学习笔记-hjimce的专栏 5. BN加在什么地方论文给出的位置是加在激活函数前，也就是“conv -&gt; bn -&gt; relu”，残差网络ResNet的代码也用的这种方式。当然，我也看到过加在激活函数后的，也就是“conv -&gt; relu -&gt; bn”。 你也可以看下这个回答：CNN中batch normalization应该放在什么位置？-lystdo的回答 6. BN是万能的？首先，提醒初学者一个容易掉进的坑： 如果BN的期望和标准差是每个mini-batch各自计算得出的，那么batch_size不要设太小，以免每个mini-batch得到的期望和标准差波动太大。 其次，不得不说，BN的效果确实显著：训练速度加快了，收敛精度也提高了。但是还是有一些情况不能使用BN：NTIRE2017夺冠的EDSR去掉了Batch Normalization层就获得了提高为什么？-pby5的回答]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>BN</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以VGG为例，分析深度网络的计算量和参数量]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fvgg-complexity%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：我第一次读到ResNet时，完全不敢相信152层的残差网络，竟然在时间复杂度（计算量）上和16层的VGG是一样大的。当然，对于初学者而言，直接分析ResNet的时间复杂度是有点难度的。这篇文章我将以VGG为例，介绍深度网络的复杂度计算方法。掌握这些计算方法后，再去看Inception、ResNet、MobileNet、SqueezeNet等论文，你就能明白这些网络结构的精妙之处。 关键字：深度网络， VGG， 复杂度分析， 计算量， 参数量 1. VGG的结构VGG的结构如下图所示： 图1 不同VGG网络的结构 我们选取其中的VGG-16（上图中的D列）来进行计算量和参数量的分析。VGG-16每个卷积操作后，图像大小变化情况如下图所示： 图2 VGG-16的结构 2. 卷积操作的计算量和参数量对于卷积操作的计算量（时间复杂度）和参数量（空间复杂度）可以看这篇文章：卷积神经网络的复杂度分析-Michael Yuan的文章 注意，这些复杂度计算都是估算，并非精确值。 我们以VGG-16的第一层卷积为例：输入图像224×224×3，输出224×224×64，卷积核大小3×3。 计算量：$$ Times\approx 224\times 224\times 3\times 3\times 3\times 64=8.7\times 10^7$$ 参数量：$$ Space\approx 3\times 3\times 3\times 64=1728$$ 再举一个例子，VGG-16的最后一个卷积层：输入14×14×512，输出14×14×512，卷积核大小3×3。 计算量：$$ Times\approx 14\times 14\times 3\times 3\times 512\times 512=4.6\times 10^8$$ 参数量：$$ Space\approx 3\times 3\times 512\times 512=2.4\times 10^6$$ 3. 全连接层的计算量和参数量考虑VGG-16的最后一个全连接层：上层神经元数为4096，下层神经元数为1000。这样的全连接层复杂度应该如何计算？ 其实全连接层可以视为一种特殊的卷积层：上层为1×1×4096，下层为1×1×1000，使用的1×1的卷积核进行卷积。 那么，计算量：$$ Times\approx 1\times 1\times 1\times 1\times 4096\times 1000=4\times 10^6$$ 参数量：$$ Space\approx 1\times 1\times 4096\times 1000=4\times 10^6$$ 4. VGG-16复杂度分析从上述计算中，相信大家对深度网络的复杂度已经有了一些体会，比如VGG-16中： 1、卷积层的时间复杂度大致是同一数量级的 2、随着网络深度加深，卷积层的空间复杂度快速上升（每层的空间复杂度是上层的两倍） 3、全连接层的空间复杂度比卷积层的最后一层还大 当然，深度网络的复杂度是和网络结构紧密相关的，上述3个结论仅对VGG这种网络结构有效。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度网络</tag>
        <tag>VGG</tag>
        <tag>复杂度分析</tag>
        <tag>计算量</tag>
        <tag>参数量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二维图像的傅立叶变换]]></title>
    <url>%2F%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2Fimage-fft%2F</url>
    <content type="text"><![CDATA[本文原载于https://imlogm.github.io，转载请注明出处~ 摘要：二维图像的傅立叶变换，与一维傅立叶相比，在理解上要抽象很多。我在网上找了几篇相对较好的文章，并用matlab自己做了几个实验图像，希望能对大家理解二维图像的傅立叶变换有所帮助。 关键字：二维傅立叶变换，图像处理 1. 一维傅立叶变换如果是理工科的话，一维傅立叶变换应该在大学里都学过。如果有所遗忘的话，可以看这篇比较易懂又不失数学性的文章“如何理解傅里叶变换公式？-马同学的回答”。 一维傅立叶变换的公式为：$$ F(\omega)=\int_{-\infty}^{+\infty}f(t)e^{-i\omega t}dt $$ 2. 二维傅立叶变换二维傅立叶变换的公式为：$$ F(u,v)=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)e^{-i(ux+vy)}dxdy $$由一维傅立叶的公式，能比较容易类比得到二维傅立叶变换公式。但注意，二维傅立叶不是x方向与y方向正弦余弦的简单叠加，而是乘积的叠加。 比如：一维傅立叶变换的三角函数系是$sin(nx)$、$cos(nx)$以及常数1，二维傅立叶变换的三角函数系是$sin(ux+vy)$、$cos(ux+vy)$和常数1.而$sin(ux+vy)$和$cos(ux+vy)$可以继续分解为$sin(ux)sin(vy),\:\:sin(ux)cos(vy),\:\:cos(ux)sin(vy),\:\:cos(ux)cos(vy)$这四个乘积的形式。 如果对上面一段话的内容有兴趣的话，可以参看这篇文章“二维傅里叶变换是怎么进行的？-CharlyGordon的回答”。看不懂也没关系，只需要知道二维傅立叶不是x方向与y方向正弦余弦的简单叠加。 注意1：二维傅立叶变换后生成的图像与原图上的像素点不存在一一对应关系。原图中的像素值是x,y坐标轴下的（即空间域），而傅立叶变换后的像素值是u,v坐标轴下的（即频域）。 注意2：图像的像素点是离散且有限的，故实际进行图像傅立叶变换时，使用的是离散傅立叶变换（DFT），需要把上述公式中的积分号$\int_{-\infty}^{+\infty}$换成求和号$\sum$ 3. 二维傅立叶变换的直观理解原始图像经过二维傅立叶变换后得到的是u,v坐标系下的二维矩阵，由$(u_1,v_1)$、$(u_1,v_2)$、$(u_2,v_1)$等一系列点组成。每个位置$(u_n,v_n)$都有其对应的值$F(u_n,v_n)$。如果把这个二维矩阵归一化成傅立叶后的图像来显示，那么傅立叶后图像上像素点位置和像素点亮度就表征二维矩阵相应的点和该点的值。 如图1所示，有左中右三幅小图，我们先不管右小图，左小图是原始图像，中小图是傅立叶变换后的。可以看到傅立叶变换后的图片的两个斜对角出现了两个白点，这两点处的亮度值最大，其余点处亮度值为0。这表示原始图像可以由这两点所对应的三角波组成，三角波的幅值为其对应点的亮度。 图1 那么这些三角波长什么样子呢？图2是我从网上找到的一幅图，原始出处未知。原来这张图是频移后的三角波，由于频移是下面才讲到的知识点，我感觉这样会对理解造成一定影响，所以我处理成频移前的，便于大家理解。相信对比图1和图2，你们能很快理解其中的关系。 图2 在图2的右小图中，我用红框标出了其中的一个三角波，这个三角波的外形与图1的原始图像最相似。结果也和我们预料的一样，傅立叶变换后，这个三角波的幅值是最大的，所以我们看到了图1中小图左上角的白点。 注意：图2的三角波对应于图1中小图的左上角的区域，所以我们得到了左上角的白点。至于图1中小图的右上、左下、右下区域的三角波长什么样子，请看下面一段话。 看到这里可能大家还有个疑问：左上角的白点明白了，但是右下角的白点怎么来的？如果你自己动手做实验的话，会发现这两个白点是对称的（换而言之，右下角区域的三角波和左上角区域的三角波对称）。这其实是由两个原因共同作用造成的：其一，傅立叶双边频谱关于原点对称；其二，上面提到过，二维图像傅立叶变换是离散傅立叶变换，离散傅里叶变换本质是周期信号求傅里叶级数，所以其实会有周期延拓。 因为这两个性质涉及一些更深的知识，我不详细展开了。有兴趣的同学可以找信号处理方面的书来看，学过的同学应该能马上理解。 解决了图1中左小图、中小图的问题，那么图1的右小图是什么呢？右小图其实是中小图经过频移后的。为什么要频移，因为我们把傅立叶变换得到的二维矩阵用图像的方式显示时，默认的坐标原点(0, 0)位于图像的左上角。频移要做的就是把坐标原点移动到图像的中心。 可以想到，图1右小图的中间偏右下的点是由图1左小图左上角的点经过移动后得到的，而图1右小图的中间偏左上的点是由之前提到的“傅立叶双边频谱关于原点对称”这条性质得到的。 4. Matlab小实验看了上面的内容，相信大家都已经对二维傅立叶有了一定的直观印象。我又用matlab写了个小程序，生成了几幅图片，帮助大家理解。 图3 图4 图5 图6 图7 图8 5. Matlab代码最后附上我的matlab代码，便于大家自己做实验。 如果对傅立叶变换的代码有什么疑问的话，可以看这篇文章“使用matlab对图像进行傅里叶变换-三山音”。12345678910111213141516171819202122232425262728293031% 用sin(x+y)的图像来帮助理解二维图像的傅立叶变换img_size = 100; % 图片尺寸x_step = 1; y_step = 1;image = zeros(img_size, img_size);for x = x_step:x_step:x_step*img_size for y=y_step:y_step:y_step*img_size image(x/x_step, y/y_step)=sin(4*pi*x/img_size + 4*pi*y/img_size); endendsubplot(1,3,1)imshow(image) % 原图title('原图')subplot(1,3,2)image = im2double(image);F_unshift = fft2(image); F_unshift_abs = abs(F_unshift);T = log(F_unshift_abs+1);imshow(T); % 傅立叶变换后，未频移前title('傅立叶变换后，未频移')subplot(1,3,3)F = fftshift(F_unshift);F_abs = abs(F);T = log(F_abs+1);imshow(T) % 傅立叶变换后，频移后title('傅立叶变换后，频移后')]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>傅立叶变换</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F%E6%B5%8B%E8%AF%95%E9%A1%B5%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>测试页</category>
      </categories>
      <tags>
        <tag>测试页</tag>
      </tags>
  </entry>
</search>
